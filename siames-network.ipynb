{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset: 370, test dataset: 30\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import random\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# prepare data\n",
    "class FaceDataset(Dataset):\n",
    "    def __init__(self, folder, trns=None):\n",
    "        super().__init__()\n",
    "        self.folder = folder\n",
    "        self.trns = trns\n",
    "        self.items = glob.glob(f\"{self.folder}/*/*\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def choose(self, items):\n",
    "        ix = random.randint(0, len(items)-1)\n",
    "        return items[ix]\n",
    "    \n",
    "    def __getitem__(self, ix):\n",
    "        imga_path = self.items[ix]\n",
    "        person = Path(imga_path).parent.name\n",
    "        same_person = random.randint(0,1)\n",
    "        if same_person:\n",
    "            imgb_path = self.choose(glob.glob(f\"{self.folder}/{person}/*\"))\n",
    "        else:\n",
    "            while True:\n",
    "                imgb_path = self.choose(self.items)\n",
    "                if Path(imgb_path).parent.name != person:\n",
    "                    break\n",
    "        \n",
    "        imga = Image.open(imga_path)\n",
    "        imgb = Image.open(imgb_path)\n",
    "        if self.trns:\n",
    "            imga = self.trns(imga)\n",
    "            imgb = self.trns(imgb)\n",
    "\n",
    "        return imga, imgb, 1-same_person\n",
    "\n",
    "train_trns = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomAffine(5, (0.01, 0.2), scale=(0.9, 1.1)),\n",
    "    T.Resize((100, 100)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.5), (0.5))\n",
    "])\n",
    "val_trns = T.Compose([\n",
    "    T.Resize((100, 100)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.5), (0.5))\n",
    "])\n",
    "\n",
    "train_ds = FaceDataset(r\"E:\\datasets\\faces\\training\", trns=train_trns)\n",
    "val_ds = FaceDataset(r\"E:\\datasets\\faces\\testing\", trns=val_trns)\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=16, shuffle=False)\n",
    "\n",
    "print(f\"train dataset: {len(train_ds)}, test dataset: {len(val_ds)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the model\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def convBlock(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        #nn.Dropout(0.2),\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            convBlock(1, 4),\n",
    "            convBlock(4, 8),\n",
    "            convBlock(8, 16),\n",
    "            convBlock(16, 32),\n",
    "            convBlock(32, 64),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64*100*100, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 32)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, imgas, imgbs):\n",
    "        fmapas = self.features(imgas)\n",
    "        fmapbs = self.features(imgbs)\n",
    "        return fmapas, fmapbs\n",
    "\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=2):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "    \n",
    "    def forward(self, features_a, features_b, label):\n",
    "        distance = F.pairwise_distance(features_a, features_b, keepdim=True)\n",
    "        loss_contrastiv = torch.mean(\n",
    "            (1-label)*(torch.pow(distance, 2)) +    # same person\n",
    "            label * (torch.pow(torch.clamp(self.margin - distance, min=0.0), 2))    # different person \n",
    "        )\n",
    "        acc = ((distance > 0.55) == label).float().mean()\n",
    "        return loss_contrastiv, acc\n",
    "\n",
    "model = SiameseNetwork().to(device)\n",
    "lossfn = ContrastiveLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, threshold=1e-6, min_lr=1e-6, threshold_mode=\"abs\", verbose=True, factor=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training utility\n",
    "\n",
    "def train_batch(model, imgas, imgbs, labels, optimizer, lossfn):\n",
    "    model.train()\n",
    "    imgas, imgbs, labels = imgas.to(device), imgbs.to(device), labels.to(device)\n",
    "    features_a, features_b = model(imgas, imgbs)\n",
    "    loss, acc = lossfn(features_a, features_b, labels)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.cpu().item(), acc.cpu().item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def val_batch(model, imgas, imgbs, labels, lossfn):\n",
    "    model.eval()\n",
    "    imgas, imgbs, labels = imgas.to(device), imgbs.to(device), labels.to(device)\n",
    "    features_a, features_b = model(imgas, imgbs)\n",
    "    loss, acc = lossfn(features_a, features_b, labels)\n",
    "    return loss.cpu().item(), acc.cpu().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 consumes 2.007722854614258s, train loss: 1.11, train acc: 0.51, test loss: 1.94, test acc: 0.46\n",
      "epoch 1 consumes 1.3383915424346924s, train loss: 1.10, train acc: 0.50, test loss: 1.47, test acc: 0.54\n",
      "epoch 2 consumes 1.3388473987579346s, train loss: 1.09, train acc: 0.50, test loss: 1.29, test acc: 0.52\n",
      "epoch 3 consumes 1.3382158279418945s, train loss: 1.07, train acc: 0.54, test loss: 1.19, test acc: 0.55\n",
      "epoch 4 consumes 1.3406031131744385s, train loss: 1.05, train acc: 0.50, test loss: 1.67, test acc: 0.41\n",
      "epoch 5 consumes 1.341254711151123s, train loss: 1.04, train acc: 0.48, test loss: 1.33, test acc: 0.55\n",
      "epoch 6 consumes 1.3364505767822266s, train loss: 1.05, train acc: 0.48, test loss: 1.30, test acc: 0.53\n",
      "epoch 7 consumes 1.3367242813110352s, train loss: 1.05, train acc: 0.52, test loss: 1.10, test acc: 0.58\n",
      "epoch 8 consumes 1.3393776416778564s, train loss: 1.03, train acc: 0.55, test loss: 1.09, test acc: 0.54\n",
      "epoch 9 consumes 1.3365466594696045s, train loss: 1.04, train acc: 0.48, test loss: 0.97, test acc: 0.59\n",
      "epoch 10 consumes 1.3384263515472412s, train loss: 1.05, train acc: 0.49, test loss: 1.22, test acc: 0.55\n",
      "epoch 11 consumes 1.3399925231933594s, train loss: 1.06, train acc: 0.52, test loss: 1.01, test acc: 0.58\n",
      "epoch 12 consumes 1.3391857147216797s, train loss: 1.01, train acc: 0.44, test loss: 1.53, test acc: 0.44\n",
      "epoch 13 consumes 1.333752155303955s, train loss: 1.05, train acc: 0.50, test loss: 1.26, test acc: 0.51\n",
      "epoch 14 consumes 1.3343420028686523s, train loss: 1.05, train acc: 0.50, test loss: 1.31, test acc: 0.48\n",
      "epoch 15 consumes 1.3330514430999756s, train loss: 1.05, train acc: 0.52, test loss: 1.11, test acc: 0.56\n",
      "epoch 16 consumes 1.3331618309020996s, train loss: 1.02, train acc: 0.43, test loss: 1.27, test acc: 0.50\n",
      "epoch 17 consumes 1.347787857055664s, train loss: 1.03, train acc: 0.44, test loss: 1.35, test acc: 0.51\n",
      "Epoch    19: reducing learning rate of group 0 to 2.5000e-04.\n",
      "epoch 18 consumes 1.3454883098602295s, train loss: 1.07, train acc: 0.56, test loss: 1.18, test acc: 0.53\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for epoch in range(200):\n",
    "    start = time.time()\n",
    "    train_losses = []\n",
    "    train_acces = []\n",
    "    val_losses = []\n",
    "    val_acces = []\n",
    "    for imgas, imgbs, labels in train_dl:\n",
    "        loss, acc = train_batch(model, imgas, imgbs, labels, optimizer, lossfn)\n",
    "        train_losses.append(loss)\n",
    "        train_acces.append(acc)\n",
    "    for imgas, imgbs, labels in val_dl:\n",
    "        loss, acc = val_batch(model, imgas, imgbs, labels, lossfn)\n",
    "        val_losses.append(loss)\n",
    "        val_acces.append(acc)\n",
    "\n",
    "    train_loss = np.mean(train_losses)\n",
    "    train_acc = np.mean(train_acces)\n",
    "    val_loss = np.mean(val_losses)\n",
    "    val_acc = np.mean(val_acces)\n",
    "    scheduler.step(train_loss)\n",
    "    end = time.time()\n",
    "    print(f\"epoch {epoch} consumes {end - start}s, train loss: {train_loss:.2f}, train acc: {train_acc:.2f}, test loss: {val_loss:.2f}, test acc: {val_acc:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f3fd24fba084f9d424971e08f1b72df2e11e336536fbb3cd19066b77485ac16c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
