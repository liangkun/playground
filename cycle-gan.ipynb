{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torchsummary import summary\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"using device: %s\" % device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vc/jzfy2dx51q32cz4_f9njz1pm0000gn/T/ipykernel_23396/1299341831.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mtest_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAppleOrangeDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapple_test_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morange_test_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mtrain_dl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mtest_dl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# map-style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             raise ValueError(\"num_samples should be a positive integer \"\n\u001b[0m\u001b[1;32m    103\u001b[0m                              \"value, but got num_samples={}\".format(self.num_samples))\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "# prepare data\n",
    "\n",
    "from appleorange import AppleOrangeDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "IMAGE_SIZE = 256\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(int(IMAGE_SIZE * 1.33)),\n",
    "    transforms.RandomCrop(IMAGE_SIZE),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "])\n",
    "\n",
    "apple_train_dir = r\"E:\\datasets\\apple-orange\\apples_train\"\n",
    "orange_train_dir = r\"E:\\datasets\\apple-orange\\oranges_train\"\n",
    "\n",
    "apple_test_dir = r\"E:\\datasets\\apple-orange\\apples_test\"\n",
    "orange_test_dir = r\"E:\\datasets\\apple-orange\\oranges_test\"\n",
    "\n",
    "train_ds = AppleOrangeDataset(apple_train_dir, orange_train_dir, transforms=transform, device=device)\n",
    "test_ds = AppleOrangeDataset(apple_test_dir, orange_test_dir, transforms=transform, device=device)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=train_ds.collate_fn)\n",
    "test_dl = DataLoader(test_ds, batch_size=5, shuffle=False, collate_fn=test_ds.collate_fn)\n",
    "\n",
    "print(\"train data: %d, test data: %d\" % (len(train_ds), len(test_ds)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare network\n",
    "def weight_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if hasattr(m, \"bias\") and m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0.0)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0.0)\n",
    "    else:\n",
    "        pass # default\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "            nn.ReLU(),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, xs):\n",
    "        return self.bolck(xs) + xs\n",
    "\n",
    "class GeneratorResNet(nn.Module):\n",
    "    def __init__(self, num_residual_blocks=9):\n",
    "        super().__init__()\n",
    "        out_features = 64\n",
    "        channels = 3\n",
    "\n",
    "        layers = [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(channels, out_features, kernel_size=7),\n",
    "            nn.InstanceNorm2d(out_features),\n",
    "            nn.ReLU()\n",
    "        ]\n",
    "\n",
    "\n",
    "        # Downsampling\n",
    "        in_features = out_features\n",
    "        for _ in range(2):\n",
    "            out_features *= 2\n",
    "            layers += [\n",
    "                nn.Conv2d(in_features, out_features, kernel_size=3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU()\n",
    "            ]\n",
    "            in_features = out_features\n",
    "        \n",
    "        # Residual blocks\n",
    "        for _ in range(num_residual_blocks):\n",
    "            layers.append(ResBlock(out_features))\n",
    "        \n",
    "        # Upsampling\n",
    "        for _ in range(2):\n",
    "            out_features //= 2\n",
    "            layers += [\n",
    "                nn.Upsample(scale_factor=2),\n",
    "                nn.Conv2d(in_features, out_features, kernel_size=3, stride=1, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU()\n",
    "            ]\n",
    "            in_features = out_features\n",
    "        \n",
    "        # Output lay\n",
    "        layers += [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(out_features, channels, kernel_size=7),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.apply(weight_init_normal)\n",
    "\n",
    "    def forward(self, xs):\n",
    "        return self.model(xs)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        channels, height, width = 3, IMAGE_SIZE, IMAGE_SIZE\n",
    "        def discriminator_block(in_channels, out_channels, normalize=True):\n",
    "            layers = [nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.InstanceNorm2d(out_channels))\n",
    "            layers.append(nn.LeakyReLU(0.2))\n",
    "\n",
    "            return layers\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(channels, 64, normalize=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128, 256),\n",
    "            *discriminator_block(256, 512),\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(512, 1, 4, padding=1)\n",
    "        )\n",
    "        self.apply(weight_init_normal)\n",
    "    \n",
    "    def forward(self, xs):\n",
    "        return self.model(xs)\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_sample(G_AB, G_BA, dl):\n",
    "    real_a, real_b = next(iter(dl))\n",
    "    G_AB.eval()\n",
    "    G_BA.eval()\n",
    "\n",
    "    fake_b = G_AB(real_a)\n",
    "    fake_a = G_BA(real_b)\n",
    "\n",
    "    real_a = make_grid(real_a, nrows=5, normalize=True)\n",
    "    real_b = make_grid(real_b, nrows=5, normalize=True)\n",
    "    fake_a = make_grid(fake_a, nrows=5, normalize=True)\n",
    "    fake_b = make_grid(fake_b, nrows=5, normalize=True)\n",
    "\n",
    "    image_grid = torch.cat([real_a, fake_b, real_b, fake_a], axis=1)\n",
    "    plt.imshow(image_grid.detach().cpu().permute(1,2,0).numpy())\n",
    "\n",
    "G_AB = GeneratorResNet().to(device)\n",
    "G_BA = GeneratorResNet().to(device)\n",
    "D_A = Discriminator().to(device)\n",
    "D_B = Discriminator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainning functions\n",
    "lambda_cycle = 10.0\n",
    "lambda_id = 5.0\n",
    "\n",
    "identity_lossfn = nn.L1Loss()\n",
    "gan_lossfn = nn.MSELoss()\n",
    "cycle_lossfn = nn.L1Loss()\n",
    "\n",
    "def train_generator_step(G_AB, G_BA, real_a, real_b, optimizer):\n",
    "    optimizer.zero_grad()\n",
    "    loss_id_a = identity_lossfn(G_BA(real_a), real_a)\n",
    "    loss_id_b = identity_lossfn(G_AB(real_b), real_b)\n",
    "    loss_id = (loss_id_a + loss_id_b) / 2\n",
    "\n",
    "    fake_b = G_AB(real_a)\n",
    "    fake_a = G_BA(real_b)\n",
    "    loss_gan_b = gan_lossfn(D_B(fake_b), torch.ones((len(real_a), 1, 16, 16)).to(device))\n",
    "    loss_gan_a = gan_lossfn(D_A(fake_a), torch.ones((len(real_b), 1, 16, 16)).to(device))\n",
    "    loss_gan = (loss_gan_a + loss_gan_b) / 2\n",
    "\n",
    "    recov_a = G_BA(fake_b)\n",
    "    recov_b = G_AB(fake_a)\n",
    "    loss_cycle_a = cycle_lossfn(recov_a, real_a)\n",
    "    loss_cycle_b = cycle_lossfn(recov_b, real_b)\n",
    "    loss_cycle = (loss_cycle_a + loss_cycle_b) / 2\n",
    "\n",
    "    loss = loss_gan + lambda_cycle * loss_cycle + lambda_id * loss_id\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss, loss_id, loss_gan, loss_cycle, fake_a, fake_b\n",
    "\n",
    "def train_discrimitor_step(D, real, fake, optimizer):\n",
    "    optimizer.zero_grad()\n",
    "    loss_real = gan_lossfn(D(real), torch.ones((len(real), 1, 16, 16)).to(device))\n",
    "    loss_fake = gan_lossfn(D(fake), torch.zeros(len(fake), 1, 16, 16).to(device))\n",
    "    loss = (loss_real + loss_fake) / 2\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vc/jzfy2dx51q32cz4_f9njz1pm0000gn/T/ipykernel_23396/191650698.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mreal_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_b\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mloss_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_gan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_cycle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_b\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mtrain_generator_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG_AB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG_BA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_G\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dl' is not defined"
     ]
    }
   ],
   "source": [
    "# trainning process\n",
    "import time\n",
    "optimizer_G = optim.Adam(itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=1e-3)\n",
    "optimizer_D_A = optim.Adam(D_A.parameters(), lr=1e-3)\n",
    "optimizer_D_B = optim.Adam(D_B.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(10):\n",
    "    start = time.time()\n",
    "    for real_a, real_b in train_dl:\n",
    "        loss_g, loss_id, loss_gan, loss_cycle, fake_a, fake_b = \\\n",
    "            train_generator_step(G_AB, G_BA, real_a, real_b, optimizer_G)\n",
    "        loss_d_a = train_discrimitor_step(D_A, real_a, fake_a, optimizer_D_A)\n",
    "        loss_d_b = train_discrimitor_step(D_B, real_b, fake_b, optimizer_D_B)\n",
    "        loss_d = (loss_d_a + loss_d_b) / 2\n",
    "    end = time.time()\n",
    "    print(\"epoch: %d, consume %ds, loss_g=%.2f, loss_d=%.2f\" % (epoch, end-start, loss_g, loss_d))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f3fd24fba084f9d424971e08f1b72df2e11e336536fbb3cd19066b77485ac16c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
